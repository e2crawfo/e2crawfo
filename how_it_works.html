

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>How It Works &mdash; nengo_mpi 0.1.0-dev docs</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="nengo_mpi 0.1.0-dev docs" href="index.html"/>
        <link rel="up" title="User Guide" href="user_guide.html"/>
        <link rel="next" title="Modules" href="modules.html"/>
        <link rel="prev" title="User Guide" href="user_guide.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> nengo_mpi
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="user_guide.html">User Guide</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="">How It Works</a></li>
<li class="toctree-l2"><a class="reference internal" href="modules.html">Modules</a></li>
<li class="toctree-l2"><a class="reference internal" href="workflows.html">Workflows</a></li>
<li class="toctree-l2"><a class="reference internal" href="benchmarks.html">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html">FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="dev_guide.html">Developer Guide</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">nengo_mpi</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
          <li><a href="user_guide.html">User Guide</a> &raquo;</li>
      
    <li>How It Works</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/how_it_works.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="how-it-works">
<span id="id1"></span><h1>How It Works<a class="headerlink" href="#how-it-works" title="Permalink to this headline">Â¶</a></h1>
<p>Here we attempt to give a rough idea of how nengo_mpi works under the hood, and, in particular, how it achieves parallelization. nengo_mpi is based heavily on the reference implementation of nengo. The reference implementation works by converting a high-level neural model specification into a low-level computation graph. The computation graph is a collection of <code class="docutils literal"><span class="pre">operators</span></code> and <code class="docutils literal"><span class="pre">signals</span></code>. In short, signals store data, and operators perform computation on signals and store the results in other signals. To run the simulation, nengo simply executes each operator in the computation graph once per time step. For a concrete example of how this works, consider the following simple nengo script:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">nengo</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Network</span><span class="p">()</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">conn</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>

<span class="n">sim</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">sim</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">time_in_seconds</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p>The conversion from the high-level specification (e.g. the nengo Network stored in the variable <code class="docutils literal"><span class="pre">model</span></code>)  to computation graph is called the <strong>build</strong> step, and takes place in the line <code class="docutils literal"><span class="pre">sim</span> <span class="pre">=</span> <span class="pre">nengo.Simulator(model)</span></code>. The generated computation graph looks something like this:</p>
<img src="_images/nengo_signals.svg" /><p>A few signals and operators whose purposes are somewhat opaque have been omitted here for clarity. Now suppose that we&#8217;re impatient and find that the call to <code class="docutils literal"><span class="pre">sim.run</span></code> is too slow. We can easily parallelize the simulation step by making use of nengo_mpi. Making the few necessary changes, we end up with the following script:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">nengo</span>
<span class="kn">import</span> <span class="nn">nengo_mpi</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Network</span><span class="p">()</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">nengo</span><span class="o">.</span><span class="n">Ensemble</span><span class="p">(</span><span class="n">n_neurons</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">dimensions</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">nengo</span><span class="o">.</span><span class="n">Connection</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>

<span class="c1"># assign the ensembles to different processors</span>
<span class="n">assignments</span> <span class="o">=</span> <span class="p">{</span><span class="n">A</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="n">B</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">nengo_mpi</span><span class="o">.</span><span class="n">Simulator</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">assignments</span><span class="o">=</span><span class="n">assignments</span><span class="p">)</span>

<span class="n">sim</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">time_in_seconds</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
<p>Now ensembles A and B will be simulated on different processors, and we should get a factor of 2 speedup in running the simulation (though it will hardly be perceptible given how tiny our network is). nengo_mpi will produce a computation graph quite similar to the one produced by vanilla nengo, except it will use operators that are implemented in C++ rather than python, and will add a few new operators to achieve the inter-process communication:</p>
<img src="_images/nengo_mpi_signals.svg" /><p>The <code class="docutils literal"><span class="pre">MPISend</span></code> operator stores the index of the processor to send its data to,
and likewise the <code class="docutils literal"><span class="pre">MPIRecv</span></code> operator stores the index of the processor to receive data from.
Moreover, they both share a &#8220;tag&#8221;, a unique identifier which bonds the two
operators together and ensures that the data from the <code class="docutils literal"><span class="pre">MPISend</span></code> operator gets
sent to the correct <code class="docutils literal"><span class="pre">MPIRecv</span></code> operator. This basic pattern can be scaled up to
simulate very large networks on thousands of processors.</p>
<p>Some readers may have noticed something odd by now: it may seem like it would
be impossible to achieve accelerated performance from the set-up depicted in
the above diagrams. In particular, it seems as if the operators on processor
1 will need to wait for the results from processor 0, so the computation is
still ultimately a serial one, just that now we have added inter-process
communication in the pipeline to slow things down.</p>
<p>This turns out not to be the case, because the <code class="docutils literal"><span class="pre">Synapse</span></code> operator is special
in that it is what we call an &#8220;update&#8221; operator. Update operators break the computation
graph up into independently-simulatable components. In the first diagram, the
<code class="docutils literal"><span class="pre">DotInc</span></code> operator in ensemble B performs computation on the value of the <code class="docutils literal"><span class="pre">Input</span></code>
signal <em>from the previous time-step</em> <a class="footnote-reference" href="#id3" id="id2">[1]</a>. Thus, the operators in ensemble B do not need to
wait for the operators in ensemble A and the connection, since the values from the
previous time-step should already be available. Likewise, in the second diagram,
the <code class="docutils literal"><span class="pre">MPIRecv</span></code> operator actually receives data from the previous time-step.
Thanks to this mechanism, we are in fact able to achieve large-scale parallelization,
demonstrated empirically by our <a class="reference internal" href="benchmarks.html#benchmarks"><span>Benchmarks</span></a>.</p>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id2">[1]</a></td><td>&#8220;Delays&#8221; like this are necessary from a biological-plausibility standpoint as well. Otherwise, neural activity elicited by a stimulus could be propogated throughout the entire network in a single time step, regardless of the network&#8217;s size.</td></tr>
</tbody>
</table>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="modules.html" class="btn btn-neutral float-right" title="Modules" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="user_guide.html" class="btn btn-neutral" title="User Guide" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2013-2014, Applied Brain Research.
      Last updated on Mar 28, 2016.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0-dev',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>